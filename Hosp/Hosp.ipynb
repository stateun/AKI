{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load admissions data\n",
    "admissions = pd.read_csv('/data/share/AKI/3.0/hosp/admissions.csv.gz', compression='gzip')\n",
    "\n",
    "# Check for missing values and handle them\n",
    "admissions.loc[admissions.admission_location.isnull(), 'admission_location'] = 'EMERGENCY ROOM'\n",
    "admissions.loc[admissions.discharge_location.isnull(), 'discharge_location'] = 'HOME'\n",
    "admissions.loc[admissions.insurance.isnull(), 'insurance'] = 'Medicare'\n",
    "admissions.loc[admissions.marital_status.isnull(), 'marital_status'] = 'MARRIED'\n",
    "\n",
    "# Add duration in hours\n",
    "def convert_to_datetime(datestr: str):\n",
    "    if ':' in datestr:\n",
    "        date_time = datetime.datetime.strptime(datestr, '%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        date_time = datetime.datetime.strptime(datestr, '%Y-%m-%d')\n",
    "    return date_time\n",
    "\n",
    "admissions['hrs'] = (\n",
    "    admissions.dischtime.apply(lambda x: convert_to_datetime(x)) - \n",
    "    admissions.admittime.apply(lambda x: convert_to_datetime(x))\n",
    ").apply(lambda x: round(x.seconds / 3600))\n",
    "\n",
    "# Select columns for the new dataset\n",
    "new_admissions = admissions.loc[:, [\n",
    "    'subject_id', 'hadm_id', 'admission_type', \n",
    "    'admission_location', 'discharge_location', \n",
    "    'insurance', 'marital_status', 'race', 'hrs'\n",
    "]]\n",
    "\n",
    "# Save the cleaned admissions data\n",
    "# os.mkdir('/data/share/AKI/3.0/hosp/new')\n",
    "# new_admissions.to_csv('/data/share/AKI/3.0/hosp/new/admissions.csv')\n",
    "\n",
    "# Load hcpcsevents and d_hcpcs data\n",
    "hcpcsevents = pd.read_csv('/data/share/AKI/3.0/hosp/hcpcsevents.csv.gz', compression='gzip')\n",
    "dhcpc = pd.read_csv('/data/share/AKI/3.0/hosp/d_hcpcs.csv.gz', compression='gzip')\n",
    "\n",
    "# Merge hcpcsevents with d_hcpcs data\n",
    "new_hcpcsevents = pd.merge(hcpcsevents, dhcpc, left_on='hcpcs_cd', right_on='code')\n",
    "\n",
    "# Load data\n",
    "diagnoses_icd = pd.read_csv('/data/share/AKI/3.0/hosp/diagnoses_icd.csv.gz', compression='gzip')\n",
    "aki_icds = ['5845', '5846', '5847', '5848', '5849', '66930', '66932', '66934', 'N17', 'N170', 'N171', 'N172', 'N178', 'N179', 'O904']\n",
    "d_icd_diagnoses = pd.read_csv('/data/share/AKI/3.0/hosp/d_icd_diagnoses.csv.gz', compression='gzip')\n",
    "\n",
    "# Filter AKI-related hadm_id and subject_id\n",
    "aki_hadms = diagnoses_icd.loc[diagnoses_icd.icd_code.apply(lambda x: x in aki_icds), 'hadm_id'].unique().tolist()\n",
    "aki_hadms = set(aki_hadms)\n",
    "\n",
    "tmp_diagnoses_icd = diagnoses_icd[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "hadm_to_subject = {str(h): s for s, h in zip(tmp_diagnoses_icd.subject_id.tolist(), tmp_diagnoses_icd.hadm_id.tolist())}\n",
    "aki_subject = [s for h, s in hadm_to_subject.items() if int(h) in aki_hadms]\n",
    "aki_subject = set(aki_subject)\n",
    "\n",
    "# Filter data before AKI occurrence\n",
    "aki_prev_hadm = []\n",
    "aki_subject_for_check = []\n",
    "for key, df in tqdm(tmp_diagnoses_icd.groupby(by=['subject_id'])):\n",
    "    first_h = list(set(df.hadm_id).intersection(aki_hadms))\n",
    "    if first_h:\n",
    "        first_h = np.array(first_h).min()\n",
    "        prev_h = df.loc[df.hadm_id <= first_h, 'hadm_id'].tolist()\n",
    "        aki_prev_hadm.extend(prev_h)\n",
    "        aki_subject_for_check.append(key)\n",
    "\n",
    "# Filter procedures_icd data\n",
    "procedures_icd = pd.read_csv('/data/share/AKI/3.0/hosp/procedures_icd.csv.gz', compression='gzip')\n",
    "\n",
    "def drop_hadms_after_aki(df):\n",
    "    df = df.loc[~df.hadm_id.isnull(), :]\n",
    "    df = df.loc[(df.subject_id.apply(lambda x: x not in aki_subject)) | \n",
    "                ((df.subject_id.apply(lambda x: x in aki_subject)) * \n",
    "                 (df.hadm_id.apply(lambda x: x in aki_prev_hadm))), :]\n",
    "    n_aki_hadms = len(set(df.hadm_id).intersection(aki_prev_hadm))\n",
    "    print(f'# unique aki hadms: {n_aki_hadms}\\n# unique hadms: {df.hadm_id.nunique()}')\n",
    "    return df\n",
    "\n",
    "procedures_icd = drop_hadms_after_aki(procedures_icd)\n",
    "\n",
    "# Calculate ICD code frequencies\n",
    "def count_after_groupby_hadm_id(df):\n",
    "    df_hadm_id = set(df.hadm_id)\n",
    "    res = []\n",
    "    print(f'There are {len(df_hadm_id)} hadm_ids in the dataframe')\n",
    "    for h, h_df in tqdm(df.groupby(by=['subject_id', 'hadm_id'])):\n",
    "        res.extend(list(set(h_df.icd_code)))\n",
    "    res = dict(pd.DataFrame(res).value_counts())\n",
    "    res = {k[0]: v for k, v in res.items()}\n",
    "    return res\n",
    "\n",
    "aki_proc_icd = procedures_icd.loc[procedures_icd.hadm_id.apply(lambda x: x in aki_prev_hadm), :]\n",
    "aki_proc_icd_cnt = count_after_groupby_hadm_id(aki_proc_icd)\n",
    "proc_icd_cnt = count_after_groupby_hadm_id(procedures_icd)\n",
    "\n",
    "# Calculate proportions and filter top ICD codes\n",
    "aki_proc_icd_prop = {icd_code: aki_proc_icd_cnt[icd_code] / proc_icd_cnt[icd_code] for icd_code in aki_proc_icd_cnt.keys()}\n",
    "aki_proc_icd_top = {k for k, v in sorted(aki_proc_icd_prop.items(), key=lambda x: -x[1]) if v > 0.5}\n",
    "\n",
    "proc_icd_top = {k for k, v in sorted(proc_icd_cnt.items(), key=lambda x: -x[1]) if v > np.mean(list(proc_icd_cnt.values()))}\n",
    "\n",
    "aki_proc_icd_top = aki_proc_icd_top.intersection(proc_icd_top)\n",
    "print(len(aki_proc_icd_top))\n",
    "\n",
    "# Loading procedures ICD file\n",
    "procedures_icd = pd.read_csv('/data/share/AKI/3.0/hosp/procedures_icd.csv.gz', compression='gzip')\n",
    "\n",
    "# Defining top AKI procedure ICD codes\n",
    "aki_proc_icd_top = [\"0014\", \"0015\", \"0092\", \"0093\", \"02H633Z\", \"9672\", \"9971\", \"B543ZZA\", \"DP081ZZ\", \"XW033H5\"]\n",
    "\n",
    "# Loading procedure metadata\n",
    "d_icd_procedures = pd.read_csv('/data/share/AKI/3.0/hosp/d_icd_procedures.csv.gz', compression='gzip')\n",
    "\n",
    "# Filtering data by top AKI ICD codes\n",
    "filtered_procedures = d_icd_procedures.loc[d_icd_procedures.icd_code.apply(lambda x: x in aki_proc_icd_top), :]\n",
    "\n",
    "# Creating a matrix for procedures ICD frequencies\n",
    "proc_icd = np.zeros((procedures_icd.hadm_id.nunique(), len(aki_proc_icd_top)))\n",
    "trow = 0\n",
    "\n",
    "for group, df in tqdm(procedures_icd.groupby(by=['subject_id', 'hadm_id'])):\n",
    "    proc_icd_i = dict(df.icd_code.value_counts())\n",
    "    proc_icd_keys = set(proc_icd_i.keys())\n",
    "    for idx, aki_proc_icd in enumerate(aki_proc_icd_top):\n",
    "        if aki_proc_icd in proc_icd_keys:\n",
    "            proc_icd[trow, idx] = proc_icd_i[aki_proc_icd]\n",
    "    trow += 1\n",
    "\n",
    "# Renaming procedure ICD columns\n",
    "proc_icd_colnames = [f'hosp_procedures_icd_{i}' for i in aki_proc_icd_top]\n",
    "proc_icd_df = pd.DataFrame(proc_icd, columns=proc_icd_colnames)\n",
    "\n",
    "# Saving processed ICD data\n",
    "proc_icd_df.to_csv('hosp/new/procedures_icd.csv', index=False)\n",
    "\n",
    "# Loading laboratory events data\n",
    "labevents = pd.read_csv('/data/share/AKI/3.0/hosp/labevents.csv.gz', compression='gzip')\n",
    "\n",
    "# Dropping NaN values for hospital admission IDs\n",
    "labevents = labevents.dropna(subset=['hadm_id'])\n",
    "\n",
    "# Loading transfer data\n",
    "transfers = pd.read_csv('/data/share/AKI/3.0/hosp/transfers.csv.gz', compression='gzip')\n",
    "\n",
    "# Saving the final dataframe\n",
    "labevents.to_csv('hosp/new/labevents_clean.csv', index=False)\n",
    "\n",
    "# Data Filtering\n",
    "labevents = labevents.loc[labevents.subject_id == 10000032, :]\n",
    "\n",
    "# Sorting and Formatting Data\n",
    "a = transfers.loc[(transfers.subject_id == 10000032) & (transfers.eventtype == 'admit'), ['hadm_id', 'intime']]\n",
    "a['intime'] = a['intime'].apply(lambda x : convert_to_datetime(x))\n",
    "a.sort_values(by='intime', inplace=True)\n",
    "\n",
    "# Helper Function\n",
    "def find_hadm(subject, charttime):\n",
    "    hadm_times = transfers.loc[(transfers.subject_id == subject) & (transfers.eventtype == 'admit'), ['hadm_id', 'intime']]\n",
    "    hadm_times['intime'] = hadm_times.intime.apply(lambda x : convert_to_datetime(x))\n",
    "    hadm_times.sort_values(by='intime', inplace=True)\n",
    "    res = hadm_times.iloc[0, 0]\n",
    "    for i in range(len(hadm_times)):\n",
    "        if hadm_times.iloc[i, 1] < charttime:\n",
    "            res = hadm_times.iloc[i, 0]\n",
    "        else:\n",
    "            return res\n",
    "    return res\n",
    "\n",
    "# Test the Function\n",
    "find_hadm(10000032, convert_to_datetime('2180-03-23 11:51:00'))\n",
    "find_hadm(10000032, convert_to_datetime('2180-06-28 12:00:00'))\n",
    "find_hadm(10000032, convert_to_datetime('2180-12-01 12:00:00'))\n",
    "\n",
    "# Filtering and Grouping Data\n",
    "labevents = labevents.loc[~labevents.hadm_id.isnull(), :]\n",
    "aki_itemids = [50912, 51006, 50971, 50983, 50902, 50882, 50868, 50931, 51221, 51265, 51222, 51301, 51249, 51248, 51250, 51279, 51277, 50960, 50893, 50970]\n",
    "\n",
    "# Creating Feature Matrices\n",
    "n_hadms = len(labevents.hadm_id.unique())\n",
    "did_itemid = np.zeros((n_hadms, len(aki_itemids)))\n",
    "is_flag = np.zeros((n_hadms, len(aki_itemids)))\n",
    "\n",
    "# Grouping and Analyzing Data\n",
    "for group, df in tqdm(labevents.groupby(by=['subject_id', 'hadm_id'])):\n",
    "    tmp_itemids = set(df.itemid)\n",
    "    for idx, tmp_itemid in enumerate(aki_itemids):\n",
    "        if tmp_itemid in tmp_itemids:\n",
    "            did_itemid[trow, idx] = 1\n",
    "            tmp_flag = (~df.loc[df.itemid == tmp_itemid, 'flag'].isnull()).any().item()\n",
    "            is_flag[trow, idx] = tmp_flag\n",
    "    trow += 1\n",
    "\n",
    "# Creating DataFrames for Feature Matrices\n",
    "did_itemid_colnames = [f'hosp_itemid_{i}' for i in aki_itemids]\n",
    "is_flag_colnames = [f'hosp_itemid_is_flag_{i}' for i in aki_itemids]\n",
    "\n",
    "did_itemid = pd.DataFrame(did_itemid, columns=did_itemid_colnames)\n",
    "is_flag = pd.DataFrame(is_flag, columns=is_flag_colnames)\n",
    "\n",
    "# Combining Results with Original Data\n",
    "new_labevents = labevents[['subject_id', 'hadm_id']].drop_duplicates().reset_index(drop=True)\n",
    "new_labevents = pd.concat([new_labevents, did_itemid, is_flag], axis=1)\n",
    "\n",
    "# Function to drop HADM_IDs after AKI\n",
    "def drop_hadms_after_aki(df):\n",
    "    df = df.loc[~df.hadm_id.isnull(), :]\n",
    "    df = df.loc[(df.subject_id.apply(lambda x: x not in aki_subject)) | \n",
    "                ((df.subject_id.apply(lambda x: x in aki_subject)) & \n",
    "                 (df.hadm_id.apply(lambda x: x in aki_prev_hadm))), :]\n",
    "    n_aki_hadms = len(set(df.hadm_id).intersection(aki_prev_hadm))\n",
    "    print(f'# unique aki hadms : {n_aki_hadms}\\n# unique hadms : {df.hadm_id.nunique()}')\n",
    "    return df\n",
    "\n",
    "# Function to count prescriptions after grouping by HADM_ID\n",
    "def count_after_groupby_hadm_id(df):\n",
    "    df_hadm_id = set(df.hadm_id)\n",
    "    res = []\n",
    "    print(f'There are {len(df_hadm_id)} HADM_IDs in the dataframe')\n",
    "    for h, h_df in tqdm(df.groupby(by=['subject_id', 'hadm_id'])):\n",
    "        res.extend(list(set(h_df.ndc)))\n",
    "    res = dict(pd.DataFrame(res).value_counts())\n",
    "    res = {k[0]: v for k, v in res.items()}\n",
    "    return res\n",
    "\n",
    "# Load the prescriptions dataset\n",
    "prescriptions = pd.read_csv('/data/share/AKI/3.0/hosp/prescriptions.csv.gz', compression='gzip')\n",
    "\n",
    "# Process prescriptions to drop HADM_IDs after AKI\n",
    "prescriptions = drop_hadms_after_aki(prescriptions)\n",
    "prescriptions = prescriptions.loc[~prescriptions.ndc.isnull(), :]\n",
    "\n",
    "# Extract AKI-related prescriptions\n",
    "aki_prescriptions = prescriptions.loc[prescriptions.hadm_id.apply(lambda x: x in aki_prev_hadm), :]\n",
    "\n",
    "# Count prescription occurrences\n",
    "aki_prescriptions_cnt = count_after_groupby_hadm_id(aki_prescriptions)\n",
    "prescriptions_cnt = count_after_groupby_hadm_id(prescriptions)\n",
    "\n",
    "# Calculate proportion of AKI prescriptions\n",
    "aki_prescriptions_prop = dict()\n",
    "for prescription in tqdm(list(aki_prescriptions_cnt.keys())):\n",
    "    aki_prescriptions_prop[prescription] = aki_prescriptions_cnt[prescription] / prescriptions_cnt[prescription]\n",
    "\n",
    "# Filter top prescriptions based on proportions\n",
    "aki_prescriptions_top = dict(sorted(aki_prescriptions_prop.items(), key=lambda x: -x[1]))\n",
    "aki_prescriptions_top = {k for k, v in aki_prescriptions_top.items() if v > 0.5}\n",
    "\n",
    "# Filter prescriptions with high overall usage\n",
    "prescriptions_top = dict(sorted(prescriptions_cnt.items(), key=lambda x: -x[1]))\n",
    "mean_prescriptions_top_val = np.array(list(prescriptions_top.values())).mean()\n",
    "prescriptions_top = {k for k, v in prescriptions_top.items() if v > mean_prescriptions_top_val}\n",
    "\n",
    "# Intersect the top prescriptions\n",
    "aki_prescriptions_top = aki_prescriptions_top.intersection(prescriptions_top)\n",
    "\n",
    "# Create a prescription matrix\n",
    "pres = np.zeros((prescriptions.hadm_id.nunique(), len(aki_prescriptions_top)))\n",
    "trow = 0\n",
    "for group, df in tqdm(prescriptions.groupby(by=['subject_id', 'hadm_id'])):\n",
    "    pres_i = dict(df.ndc.value_counts())\n",
    "    pres_keys = set(pres_i.keys())\n",
    "    for idx, aki_prescription in enumerate(aki_prescriptions_top):\n",
    "        if aki_prescription in pres_keys:\n",
    "            pres[trow, idx] = pres_i[aki_prescription]\n",
    "    trow += 1\n",
    "\n",
    "# Create a DataFrame with prescription data\n",
    "pres_colnames = [f'hosp_prescriptions_{i}' for i in aki_prescriptions_top]\n",
    "pres = pd.DataFrame(pres, columns=pres_colnames)\n",
    "\n",
    "# Merge with subject and HADM_ID information\n",
    "new_prescriptions = prescriptions[['subject_id', 'hadm_id']].drop_duplicates().reset_index(drop=True)\n",
    "final_prescriptions = pd.concat([new_prescriptions, pres], axis=1)\n",
    "\n",
    "# Save the processed data\n",
    "new_data_dir = './hosp/new'\n",
    "if not os.path.exists(new_data_dir):\n",
    "    os.makedirs(new_data_dir)\n",
    "final_prescriptions.to_csv(f'{new_data_dir}/processed_prescriptions.csv', index=False)\n",
    "\n",
    "# Load microbiology data with gzip compression\n",
    "microbiology = pd.read_csv('/data/share/AKI/3.0/hosp/microbiologyevents.csv.gz', compression='gzip')\n",
    "\n",
    "# Filter out rows where org_itemid is NaN\n",
    "microbiology = microbiology.loc[~microbiology.org_itemid.isnull(), :]\n",
    "\n",
    "# Select specific columns from the microbiology DataFrame\n",
    "microbiology = microbiology[['subject_id', 'hadm_id', 'test_itemid', 'org_itemid', 'ab_itemid', 'dilution_value', 'interpretation']]\n",
    "\n",
    "# Filter rows where ab_itemid is not null\n",
    "microbiology = microbiology.loc[~microbiology.ab_itemid.isnull(), :]\n",
    "\n",
    "# Create a composite column for unique test identifiers\n",
    "microbiology['test_org_ab_itemid'] = (\n",
    "    microbiology['test_itemid'].apply(lambda x: str(int(x))) + '/' +\n",
    "    microbiology['org_itemid'].apply(lambda x: str(int(x))) + '/' +\n",
    "    microbiology['ab_itemid'].apply(lambda x: str(int(x)))\n",
    ")\n",
    "\n",
    "# Filter for AKI-related microbiology data\n",
    "aki_microbiology = microbiology.loc[\n",
    "    microbiology.hadm_id.apply(lambda x: x in aki_prev_hadm), :\n",
    "]\n",
    "\n",
    "# Count the occurrences of unique test identifiers grouped by hadm_id\n",
    "def count_after_groupby_hadm_id(df):\n",
    "    df_hadm_id = set(df.hadm_id)\n",
    "    res = []\n",
    "    for h, h_df in tqdm(df.groupby(by=['subject_id', 'hadm_id'])):\n",
    "        res.extend(list(set(h_df.test_org_ab_itemid)))\n",
    "    res = dict(pd.DataFrame(res).value_counts())\n",
    "    res = {k[0]: v for k, v in res.items()}\n",
    "    return res\n",
    "\n",
    "aki_microbiology_cnt = count_after_groupby_hadm_id(aki_microbiology)\n",
    "microbiology_cnt = count_after_groupby_hadm_id(microbiology)\n",
    "\n",
    "# Calculate proportion of AKI-specific microbiology tests\n",
    "aki_microbiology_prop = {\n",
    "    i: aki_microbiology_cnt[i] / microbiology_cnt[i]\n",
    "    for i in aki_microbiology_cnt.keys()\n",
    "}\n",
    "\n",
    "# Identify the top AKI-related microbiology tests\n",
    "aki_microbiology_top = dict(\n",
    "    sorted(aki_microbiology_prop.items(), key=lambda x: -x[1])\n",
    ")\n",
    "aki_microbiology_top = {k for k, v in aki_microbiology_top.items() if v > 0.6}\n",
    "\n",
    "# Identify microbiology tests with occurrences above the mean\n",
    "mean_microbiology_top_val = np.array(list(microbiology_cnt.values())).mean()\n",
    "microbiology_top = {k for k, v in microbiology_cnt.items() if v > mean_microbiology_top_val}\n",
    "\n",
    "# Filter tests that are both AKI-related and frequently occurring\n",
    "aki_microbiology_top = aki_microbiology_top.intersection(microbiology_top)\n",
    "\n",
    "# Create a matrix for microbiology test results\n",
    "mic = np.zeros((microbiology.hadm_id.nunique(), len(aki_microbiology_top)))\n",
    "trow = 0\n",
    "for group, df in tqdm(microbiology.groupby(by=['subject_id', 'hadm_id'])):\n",
    "    mic_i = df[['test_org_ab_itemid', 'interpretation']]\n",
    "    mic_keys = set(mic_i.test_org_ab_itemid)\n",
    "    for idx, tmp_mic in enumerate(aki_microbiology_top):\n",
    "        if tmp_mic in mic_keys:\n",
    "            val = mic_i.loc[mic_i.test_org_ab_itemid == tmp_mic, 'interpretation'].tolist()[0]\n",
    "            if val == 'S':\n",
    "                val = 1\n",
    "            elif val == 'I':\n",
    "                val = 2\n",
    "            elif val == 'R':\n",
    "                val = 3\n",
    "            else:\n",
    "                val = 0\n",
    "            mic[trow, idx] = val\n",
    "    trow += 1\n",
    "\n",
    "# Processing microbiology data\n",
    "mic_colnames = [f'microbiology_{i}' for i in aki_microbiology_top]  # Generate new column names\n",
    "mic = pd.DataFrame(mic)  # Convert mic to a DataFrame\n",
    "mic.columns = mic_colnames  # Assign the new column names\n",
    "\n",
    "# Remove duplicates and reset the index for the existing microbiology DataFrame\n",
    "new_microbiology = microbiology[['subject_id', 'hadm_id']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Combine the new columns and save to CSV\n",
    "new_microbiology = pd.concat((new_microbiology, mic), axis=1)\n",
    "new_microbiology.to_csv('hosp/new/microbiologyevents.csv')\n",
    "\n",
    "# Processing patients data\n",
    "patients = pd.read_csv('/data/share/AKI/3.0/hosp/patients.csv.gz', compression='gzip')  # Load the data\n",
    "patients = patients[['subject_id', 'gender', 'anchor_age', 'dod']]  # Select required columns\n",
    "\n",
    "# Add a new column indicating whether the patient has died\n",
    "patients['died'] = ~patients.dod.isnull()\n",
    "\n",
    "# Drop unnecessary columns and verify\n",
    "patients = patients.drop('dod', axis=1)  # Drop the 'dod' column\n",
    "patients.isnull().sum()  # Check for missing values\n",
    "\n",
    "# Save the processed data to CSV\n",
    "patients.to_csv('hosp/new/patients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
